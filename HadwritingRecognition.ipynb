{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(test_split=0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "(25000L,)\n",
      "(25000L,)\n"
     ]
    }
   ],
   "source": [
    "#Summarize data\n",
    "print \"Training Data\"\n",
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print \"Classes\"\n",
    "print np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words\n",
      "102099\n"
     ]
    }
   ],
   "source": [
    "print \"number of words\"\n",
    "print len(np.unique(np.hstack(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length..\n",
      "mean 285.84, words (212.622320)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6HEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18b9d4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Summarize review length\n",
    "#We can see that the average review has just under 300 words with a standard deviation of just over 200 words.\n",
    "print \"Review length..\"\n",
    "result = map(len, X_train)\n",
    "print(\"mean %.2f, words (%f)\" % (np.mean(result), np.std(result)))\n",
    "\"Plot review length\"\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking a box and whisker plot for the review lengths in words, we can probably see an exponential distribution that we can probably cover the mass of the distribution with a clipped length of 400 to 500 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recent breakthrough in the field of natural language processing is called word embedding.\n",
    "\n",
    "This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
    "\n",
    "Discrete words are mapped to vectors of continuous numbers. This is useful when working with natural language problems with neural networks and deep learning models are we require numbers as input.\n",
    "\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
    "\n",
    "The layer takes arguments that define the mapping including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension.\n",
    "\n",
    "We would like to use a word embedding representation for the IMDB dataset.\n",
    "\n",
    "Let’s say that we are only interested in the first 5,000 most used words in the dataset. Therefore our vocabulary size will be 5,000. We can choose to use a 32-dimension vector to represent each word. Finally, we may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0 values.\n",
    "\n",
    "We would load the IMDB dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([ [1, 20, 28, 716, 48, 495, 79, 27, 493, 8, 2, 7, 50, 5, 4682, 2, 10, 5, 852, 157, 11, 5, 1716, 3351, 10, 5, 500, 2, 6, 33, 256, 41, 2, 7, 17, 23, 48, 1537, 3504, 26, 269, 929, 18, 2, 7, 2, 4284, 8, 105, 5, 2, 182, 314, 38, 98, 103, 7, 36, 2184, 246, 360, 7, 19, 396, 17, 26, 269, 929, 18, 1769, 493, 6, 116, 7, 105, 5, 575, 182, 27, 5, 1002, 1085, 130, 62, 17, 24, 89, 17, 13, 381, 1421, 8, 2, 7, 5, 2723, 38, 325, 7, 17, 23, 93, 9, 156, 252, 19, 235, 20, 28, 5, 104, 76, 7, 17, 169, 35, 2, 17, 23, 1460, 7, 36, 2184, 934, 56, 2134, 6, 17, 891, 214, 11, 5, 1552, 6, 92, 6, 33, 256, 82, 7],\n",
       "         [1, 621, 6, 135, 101, 84, 392, 27, 20, 133, 1522, 63, 2, 2, 896, 11, 213, 149, 9, 417, 180, 1748, 32, 63, 31, 525, 7, 78, 42, 147, 66, 644, 113, 89, 8, 21, 147, 89, 13, 1851, 3994, 43, 170, 6, 60, 21, 296, 35, 1310, 214, 6, 789, 2, 7, 15, 16, 12, 14, 15, 16, 12, 14, 5, 927, 10, 5, 2, 13, 62, 23, 652, 25, 2046, 927, 7, 73, 574, 49, 5, 2, 1264, 56, 135, 46, 38, 6, 5, 191, 342, 10, 2, 10, 2, 2, 2572, 13, 627, 7, 412, 18, 361, 6, 20, 2, 342, 2, 45, 241, 382, 5, 28, 7, 15, 16, 12, 14, 15, 16, 12, 14, 5, 132, 18, 5, 28, 24, 3773, 209, 6, 2380, 61, 6, 2082, 146, 2, 6, 2962, 146, 1003, 6, 523, 146, 910, 6, 99, 7, 19, 165, 266, 53, 23, 460, 6, 29, 33, 199, 190, 11, 41, 286, 2, 11, 186, 17, 7, 5, 78, 1522, 24, 89, 33, 4317, 17, 551, 1851, 3994, 43, 37, 240, 40, 635, 9, 189, 331, 4183, 45, 5, 2, 6, 102, 37, 24, 5, 137, 18, 5, 757, 7, 15, 16, 12, 14, 15, 16, 12, 14, 25, 26, 212, 63, 20, 30, 13, 36, 11, 41, 635, 636, 7, 53, 230, 35, 212, 43, 46, 199, 2, 26, 2539, 61, 1401, 7, 5, 453, 4693, 231, 112, 40, 93, 4232, 27, 9, 2, 2, 56, 127, 7, 78, 124, 20, 30, 18, 9, 3245, 617, 2806, 6, 911, 19, 66, 82, 64, 681, 4058, 11, 5, 1324, 7],\n",
       "         [1, 20, 28, 24, 534, 7, 19, 312, 17, 316, 229, 6, 8, 36, 71, 5, 242, 172, 7, 17, 23, 57, 1165, 11, 124, 6, 5, 79, 384, 1872, 6, 17, 23, 2240, 6, 8, 1238, 2274, 7, 19, 107, 35, 930, 1031, 26, 248, 6, 51, 60, 33, 202, 11, 141, 155, 17, 23, 51, 2274, 6, 82, 17, 26, 631, 7, 5, 78, 65, 172, 58, 20, 28, 24, 317, 3471, 6, 39, 434, 45, 5, 471, 6, 8, 64, 183, 538, 18, 5, 28, 7, 5, 133, 543, 18, 20, 28, 13, 634, 6, 8, 5, 242, 904, 13, 2, 7, 19, 235, 20, 28, 45, 28, 2, 208, 853, 6, 51, 19, 2, 17, 74, 41, 9, 65, 28, 7, 1821, 2961, 6, 24, 19, 376, 43, 43, 43, 43, 5, 132, 13, 44, 534, 6, 8, 5, 1011, 425, 13, 849, 8, 1585, 7, 177, 172, 13, 6, 2, 2, 6, 5, 97, 309, 123, 6, 1455, 35, 93, 18, 159, 99, 119, 18, 5, 511, 6, 18, 211, 6, 69, 24, 18, 64, 204, 65, 675, 7, 191, 27, 317, 3471, 7, 20, 28, 1872, 43],\n",
       "         ...,\n",
       "         [1, 20, 13, 977, 9, 28, 45, 44, 6, 29, 262, 9, 168, 2, 140, 6, 833, 26, 5, 108, 194, 22, 18, 2, 22, 6, 8, 1213, 64, 10, 5, 832, 892, 410, 10, 5, 276, 7, 22, 1127, 18, 5, 2, 22, 13, 48, 444, 495, 385, 620, 21, 240, 41, 2, 41, 266, 11, 2664, 146, 3012, 2, 43],\n",
       "         [1, 20, 2, 1160, 231, 12, 1041, 2, 938, 607, 2, 25, 9, 2, 2350, 49, 13, 2221, 47, 5, 2670, 10, 5, 95, 10, 4110, 32, 43, 31, 11, 644, 48, 465, 2, 32, 1782, 1415, 2, 317, 3799, 31, 50, 778, 9, 2, 2, 2325, 778, 2, 50, 48, 2, 11, 2594, 5, 201, 7, 33, 207, 44, 21, 63, 483, 159, 9, 2, 134, 6, 20, 13, 181, 200, 9, 2750, 8, 5, 2, 10, 4110, 2, 45, 17, 27, 2, 7, 1240, 6, 5, 108, 32, 4631, 31, 1524, 172, 58, 20, 529, 13, 5, 3331, 2, 10, 2, 2, 23, 2, 3066, 1033, 765, 26, 5, 651, 2993, 7, 5, 108, 3350, 1524, 243, 2242, 9, 154, 18, 9, 3493, 21, 13, 2, 627, 1429, 2, 7, 395, 175, 2, 875, 2, 486, 11, 94, 44, 2, 2, 34, 197, 18, 5, 4836, 10, 39, 637, 63, 19, 24, 190, 11, 2, 20, 3799, 23, 265, 30, 34, 39, 2, 29, 111, 19, 235, 3551, 23, 2, 24, 3522, 34, 53, 7, 19, 74, 98, 52, 11, 2, 2, 21, 19, 258, 257, 5, 78, 422, 18, 5, 493, 10, 5, 201, 11, 101, 9, 1699, 840, 10, 20, 8, 2419, 23, 2, 7],\n",
       "         [1, 3873, 4435, 122, 1279, 20, 24, 563, 2, 19, 1814, 18, 5, 752, 26, 26, 48, 580, 8, 770, 249, 8, 19, 222, 19, 24, 190, 11, 2, 61, 72, 530, 91, 18, 5, 2, 2, 2, 3190, 2, 20, 28, 1007, 73, 67, 1100, 88, 248, 246, 47, 9, 780, 423, 30, 2, 602, 613, 146, 294, 6, 20, 28, 66, 1450, 33, 2, 899, 33, 367, 2, 8, 2724, 18, 28, 2, 33, 141, 6, 2, 6, 721, 146, 1003, 8, 115, 203, 2, 33, 96, 41, 5, 78, 675, 53, 572, 7, 1947, 2, 2, 20, 2, 140, 7, 15, 16, 12, 14, 15, 16, 12, 14, 2, 41, 2790, 7, 15, 16, 12, 14, 15, 16, 12, 14, 19, 396, 36, 167, 20, 7, 15, 16, 12, 14, 15, 16, 12, 14, 4774, 7, 15, 16, 12, 14, 15, 16, 12, 14, 105, 38, 33, 63, 15, 16, 12, 14, 15, 16, 12, 14, 19, 165, 204, 65, 7, 15, 16, 12, 14, 15, 16, 12, 14, 2895, 20, 276, 63, 15, 16, 12, 14, 15, 16, 12, 14, 19, 258, 7, 15, 16, 12, 14, 15, 16, 12, 14, 20, 925, 24, 2, 229, 67, 268, 88, 4524, 11, 124, 20, 2, 3230, 20, 13, 632, 7]], dtype=object),\n",
       "  array([1, 0, 0, ..., 1, 0, 0])),\n",
       " (array([ [1, 9, 1824, 3144, 18, 2, 13, 5, 970, 26, 20, 22, 509, 5, 2, 22, 30, 77, 13, 1217, 6, 183, 6, 466, 6, 8, 2285, 7, 277, 17, 393, 2, 18, 681, 30, 2, 6, 22, 5, 220, 99, 4945, 22, 24, 36, 647, 11, 2, 10, 2211, 648, 853, 6, 29, 24, 2, 18, 304, 819, 6, 8, 627, 18, 2211, 2, 7, 15, 16, 12, 14, 15, 16, 12, 14, 17, 13, 2364, 21, 22, 5, 220, 99, 4945, 22, 131, 83, 5, 2, 3418, 10, 9, 357, 797, 6, 1065, 105, 128, 2, 357, 59, 647, 20, 344, 6, 159, 25, 22, 19, 141, 49, 569, 86, 7, 22, 22, 5, 220, 99, 4945, 6, 22, 9, 2042, 65, 1707, 30, 6, 59, 83, 11, 4945, 2, 34, 109, 221, 7, 15, 16, 12, 14, 15, 16, 12, 14, 2144, 6, 18, 22, 5, 220, 99, 4945, 6, 22, 527, 2, 2, 13, 3268, 8, 868, 25, 4144, 6, 5, 2, 237, 2, 1704, 49, 36, 78, 13, 130, 2, 47, 55, 629, 29, 98, 13, 58, 11, 1577, 55, 311, 6, 8, 643, 75, 679, 9, 2, 34, 9, 180, 139, 7, 274, 1973, 292, 1775, 2, 49, 13, 2, 313, 2, 6, 5, 2, 6, 4658, 4945, 1704, 49, 284, 5, 1800, 3144, 1968, 11, 351, 39, 1523, 3017, 81, 3511, 6, 8, 2107, 37, 66, 851, 50, 39, 1523, 7, 204, 6, 1293, 2, 2, 32, 138, 19, 57, 82, 55, 18, 9, 2015, 2188, 63, 31, 2, 313, 2, 6, 5, 2, 49, 507, 127, 2662, 6, 42, 25, 9, 2, 3171, 6, 5, 97, 25, 9, 2, 2, 7, 55, 123, 1543, 5, 2, 26, 666, 2883, 5, 3147, 6, 29, 13, 5, 241, 4363, 10, 5, 121, 6, 102, 10, 9, 591, 10, 9, 22, 164, 79, 22, 56, 3616, 25, 11, 155, 69, 142, 62, 69, 142, 7, 2, 13, 205, 214, 11, 148, 446, 45, 364, 27, 9, 2, 6, 2, 1146, 578, 49, 2, 34, 55, 7, 210, 6, 55, 824, 38, 36, 627, 11, 41, 585, 27, 55, 139, 18, 117, 114, 6, 77, 206, 1058, 7, 53, 23, 73, 3713, 445, 56, 4612, 338, 144, 11, 1253, 155, 9, 1258, 6, 1540, 261, 50, 9, 2, 2, 74, 2219, 11, 41, 9, 2, 2, 3171, 49, 2, 762, 26, 2, 2, 4828, 7, 15, 16, 12, 14, 15, 16, 12, 14, 483, 20, 2, 6, 22, 5, 220, 99, 4945, 22, 2846, 6, 18, 1063, 194, 6, 102, 17, 13, 9, 687, 6, 255, 65, 30, 7, 15, 16, 12, 14, 15, 16, 12, 14, 25, 42, 251, 542, 6, 2775, 11, 4945, 13, 5, 2, 26, 139, 7, 1110, 17, 23, 145, 11, 1847, 18, 27, 215, 2166, 6, 8, 94, 18, 5, 4945, 6, 262, 88, 885, 34, 5, 2, 6, 102, 139, 577, 11, 33, 572, 7, 15, 16, 12, 14, 15, 16, 12, 14, 5, 220, 99, 4945, 13, 9, 103, 1320, 28, 6, 9, 103, 2, 28, 6, 9, 103, 22, 62, 759, 35, 19, 129, 22, 28, 6, 8, 358, 52, 17, 25, 92, 25, 370, 7],\n",
       "         [1, 2, 5, 137, 307, 328, 10, 2, 2, 63, 143, 110, 7, 92, 6, 1234, 6, 5, 134, 13, 36, 51, 579, 8, 3560, 25, 18, 5, 289, 6, 528, 2, 8, 2, 109, 847, 788, 2, 2, 7, 29, 5, 30, 13, 80, 8, 1294, 22, 682, 22, 6, 59, 48, 2, 8, 457, 6, 21, 97, 2038, 90, 27, 2, 8, 5, 180, 42, 50, 2, 90, 420, 591, 7, 17, 59, 5, 1013, 10, 4145, 2, 6, 5, 926, 10, 2, 2, 8, 2, 2, 7, 45, 241, 19, 281, 17, 59, 31, 7, 2, 6, 2, 8, 2, 138, 336, 311, 18, 2220, 81, 121, 7, 2, 23, 2, 206, 11, 41, 5, 1117, 1027, 10, 5, 198, 32, 298, 595, 2, 49, 13, 5, 42, 10, 5, 832, 174, 143, 31, 8, 2, 5, 70, 3277, 10, 20, 1695, 201, 32, 2, 31, 7, 21, 23, 17, 7, 21, 13, 5, 114, 17, 582, 6, 501, 89, 31, 31, 31, 2],\n",
       "         [1, 17, 13, 48, 238, 4187, 741, 2161, 43, 240, 82, 26, 44, 2063, 43, 29, 5, 668, 2346, 328, 13, 36, 116, 65, 7, 395, 5, 4254, 328, 96, 41, 145, 7],\n",
       "         ...,\n",
       "         [1, 71, 18, 9, 99, 30, 6, 53, 13, 639, 64, 1643, 840, 6, 162, 21, 33, 66, 153, 436, 17, 24, 409, 6, 29, 53, 24, 21, 254, 6, 56, 21, 194, 10, 5, 245, 6, 56, 21, 333, 958, 6, 20, 24, 57, 343, 409, 44, 149, 7, 5, 132, 24, 1310, 6, 5, 245, 409, 6, 605, 27, 128, 2, 2, 45, 2, 23, 2053, 6, 8, 71, 5, 333, 326, 85, 2, 45, 137, 7, 20, 24, 9, 70, 99, 30, 8, 42, 21, 71, 2083, 4781, 3037, 24, 2, 50, 493, 7, 124, 17, 60, 33, 202, 11, 87, 9, 31, 2681, 2463, 1355, 2, 2, 920, 31, 82, 57, 105, 99, 9, 30, 66, 41, 7, 20, 13, 42, 30, 135, 19, 66, 375, 5, 747, 391, 22, 53, 23, 2, 249, 10, 72, 139, 19, 96, 131, 94, 164, 22, 27, 64, 2, 43],\n",
       "         [1, 5, 28, 78, 2495, 5, 456, 18, 2, 20, 344, 32, 4002, 31, 6, 127, 173, 118, 17, 23, 4129, 797, 6, 8, 118, 128, 4661, 304, 23, 83, 273, 109, 114, 11, 5, 1050, 7, 689, 5, 2, 10, 5, 4661, 304, 23, 500, 11, 5, 797, 100, 5, 2211, 6, 27, 148, 1050, 608, 11, 124, 7, 15, 16, 12, 14, 15, 16, 12, 14, 5, 28, 13, 9, 103, 1711, 6, 2308, 876, 1229, 100, 146, 334, 18, 48, 2, 114, 7, 18, 9, 528, 2, 114, 6, 5, 79, 13, 594, 58, 9, 752, 587, 49, 4812, 9, 79, 26, 39, 752, 6, 252, 5, 1946, 2, 752, 313, 13, 162, 37, 663, 374, 7, 15, 16, 12, 14, 15, 16, 12, 14, 45, 104, 6, 5, 322, 251, 41, 9, 133, 243, 1461, 58, 77, 79, 84, 38, 1046, 6, 29, 68, 5, 79, 4157, 6, 84, 82, 21, 5, 136, 218, 9, 941, 428, 8, 9, 678, 2837, 826, 11, 9, 103, 79, 26, 9, 180, 2, 752, 438, 7, 15, 16, 12, 14, 15, 16, 12, 14, 5, 322, 13, 330, 18, 5, 484, 60, 20, 13, 9, 79, 21, 13, 437, 11, 80, 40, 582, 18, 876, 2, 8, 2, 6, 29, 21, 169, 35, 574, 7, 15, 16, 12, 14, 15, 16, 12, 14, 5, 114, 5, 79, 13, 594, 27, 9, 136, 26, 752, 6, 2807, 6, 3990, 2472, 6, 499, 8, 1506, 6, 184, 20, 9, 103, 1358, 34, 5, 2, 10, 876, 456, 25, 92, 25, 752, 7, 15, 16, 12, 14, 15, 16, 12, 14, 298, 631, 160, 352, 68, 33, 124, 20, 28, 6, 365, 11, 82, 17, 18, 9, 456, 321, 10, 34, 146, 709, 45, 364, 7, 15, 16, 12, 14, 15, 16, 12, 14, 42, 2809, 237, 171, 87, 5, 813, 13, 528, 9, 133, 243, 634, 7, 171, 108, 10, 17, 13, 103, 244, 6, 53, 38, 9, 189, 398, 18, 5, 28, 21, 19, 120, 46, 161, 40, 2221, 64, 67, 934, 244, 7, 29, 295, 5, 211, 21, 5, 79, 1346, 398, 10, 499, 110, 5, 175, 2219, 26, 2, 398, 18, 244, 25, 92, 7],\n",
       "         [1, 19, 1542, 518, 2, 23, 2134, 659, 68, 17, 394, 61, 6, 8, 24, 2653, 68, 19, 235, 5, 1488, 26, 20, 30, 6, 8, 71, 67, 51, 68, 19, 273, 21, 2, 83, 550, 17, 327, 7, 5, 30, 6, 210, 6, 24, 9, 681, 2, 7, 5, 2050, 38, 1280, 811, 5, 1035, 8, 2533, 2, 10, 5, 219, 2, 6, 8, 5, 79, 13, 887, 594, 7, 2, 206, 11, 40, 2221, 11, 1158, 34, 5, 67, 2, 1403, 10, 5, 2134, 659, 6, 8, 37, 169, 35, 101, 9, 70, 65, 311, 45, 17, 6, 360, 7, 15, 16, 12, 14, 15, 16, 12, 14, 5, 108, 749, 194, 10, 5, 219, 2134, 659, 24, 5, 2, 653, 218, 2, 8, 2, 7, 46, 85, 215, 61, 10, 81, 225, 76, 8, 288, 6, 914, 314, 47, 2, 7, 108, 10, 44, 6, 46, 85, 183, 8, 1456, 7, 36, 51, 144, 7, 2, 59, 73, 2, 1622, 1681, 6, 8, 2, 13, 9, 2, 2, 49, 57, 503, 11, 94, 3295, 7, 71, 171, 5, 30, 13, 801, 6, 2, 169, 35, 40, 11, 41, 43, 15, 16, 12, 14, 15, 16, 12, 14, 84, 40, 44, 129, 125, 84, 518, 6, 29, 589, 35, 396, 11, 315, 6, 26, 64, 299, 56, 97, 7, 19, 589, 35, 396, 2, 11, 269, 6, 561, 295, 25, 9, 1718, 36, 11, 2, 146, 690, 8, 4738, 7, 2, 23, 9, 1262, 2, 6, 29, 548, 36, 9, 1262, 10, 193, 1108, 3420, 7]], dtype=object),\n",
       "  array([1, 1, 1, ..., 0, 1, 0])))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data(nb_words=5000, test_split=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then use the Keras utility to truncate or pad the dataset to a length of 500 for each observation using the sequence.pad_sequences() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, later on, the first layer of our model would be an word embedding layer created using the Embedding class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0xd453828>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.embeddings.Embedding(5000, 32, input_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Multi-Layer Perceptron Model for the IMDB Dataset\n",
    "\n",
    "We can start off by developing a simple multi-layer perceptron model with a single hidden layer.\n",
    "\n",
    "The word embedding representation is a true innovation and we will demonstrate what would have been considered world class results in 2011 with a relatively simple neural network.\n",
    "\n",
    "Let’s start off by importing the classes and functions required for this model and initializing the random number generator to a constant value to ensure we can easily reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "test_split = 0.33\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words, test_split=test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model. We will use an Embedding layer as the input layer, setting the vocabulary to 5,000, the word vector size to 32 dimensions and the input_length to 500. The output of this first layer will be a 32×500 sized matrix as discussed in the previous section.\n",
    "\n",
    "We will flatten the Embedded layers output to one dimension, then use one dense hidden layer of 250 units with a rectifier activation function. The output layer has one neuron and will use a sigmoid activation to output values of 0 and 1 as predictions.\n",
    "\n",
    "The model uses logarithmic loss and is optimized using the efficient ADAM optimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 500, 32)       160000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 250)           4000250     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             251         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4160501\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit the model and use the test set as validation while training. This model overfits very quickly so we will use very few training epochs, in this case just 2.\n",
    "\n",
    "There is a lot of data so we will use a batch size of 128. After the model is trained, we evaluate it’s accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16750 samples, validate on 8250 samples\n",
      "Epoch 1/2\n",
      "16750/16750 [==============================] - 70s - loss: 0.5616 - acc: 0.6785 - val_loss: 0.3420 - val_acc: 0.8573\n",
      "Epoch 2/2\n",
      "16750/16750 [==============================] - 70s - loss: 0.2027 - acc: 0.9219 - val_loss: 0.3299 - val_acc: 0.8627\n",
      "Accuracy: 86.27%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example fits the model and summarizes the estimated performance. We can see that this very simple model achieves a score of 86.27% which is in the neighborhood of the original paper, with very little effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’m sure we can do better if we trained this network, perhaps using a larger embedding and adding more hidden layers. Let’s try a different network type.\n",
    "\n",
    "One-Dimensional Convolutional Neural Network Model for the IMDB Dataset\n",
    "\n",
    "Convolutional neural networks were designed to honor the spatial structure in image data whilst being robust to the position and orientation of learned objects in the scene.\n",
    "\n",
    "This same principle can be used on sequences, such as the one-dimensional sequence of words in a movie review. The same properties that make the CNN model attractive for learning to recognize objects in images can help to learn structure in paragraphs of words, namely the techniques invariance to the specific position of features.\n",
    "\n",
    "Keras supports one dimensional convolutions and pooling by the Convolution1D and MaxPooling1D classes respectively.\n",
    "\n",
    "Again, let’s import the classes and functions needed for this example and initialize our random number generator to a constant value so that we can easily reproduce results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "test_split = 0.33\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words, test_split=test_split)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our convolutional neural network model. This time, after the Embedding input layer, we insert a Convolution1D layer. This convolutional layer has 32 feature maps and reads embedded word representations 3 vector elements of the word embedding at a time.\n",
    "\n",
    "The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 500, 32)       160000      embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 500, 32)       3104        embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 250, 32)       0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 8000)          0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 250)           2000250     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             251         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2163605\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gof.opt): Optimization failure due to: local_abstractconv_check\n",
      "ERROR (theano.gof.opt): node: AbstractConv2d{border_mode='half', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(None, None, None, None)}(DimShuffle{0,2,1,x}.0, convolution1d_1_W)\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.py\", line 1772, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"C:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\tensor\\nnet\\opt.py\", line 402, in local_abstractconv_check\n",
      "    node.op.__class__.__name__)\n",
      "AssertionError: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both \"conv_dnn\" and \"conv_gemm\" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against?\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both \"conv_dnn\" and \"conv_gemm\" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4f685bddf787>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1024\u001b[0m                                                                            \u001b[0mcheck_batch_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m                                                                            batch_size=batch_size)\n\u001b[1;32m-> 1026\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[0mval_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_make_test_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    693\u001b[0m                                             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m                                             \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m                                             **self._function_kwargs)\n\u001b[0m\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_predict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Invalid argument '%s' passed to K.function\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m                                         \u001b[0mallow_input_downcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                                         \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                                         **kwargs)\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\compile\\function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    318\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\compile\\pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    477\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1774\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1776\u001b[1;33m                    \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1777\u001b[0m             defaults)\n\u001b[0;32m   1778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[0;32m   1454\u001b[0m                         optimizer, inputs, outputs)\n\u001b[0;32m   1455\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1456\u001b[1;33m                     \u001b[0moptimizer_profile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \"\"\"\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                 \u001b[0msub_prof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[0msub_profs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph, start_from)\u001b[0m\n\u001b[0;32m   1877\u001b[0m                     \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1878\u001b[0m                 \u001b[0mcurrent_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1879\u001b[1;33m                 \u001b[0mnb\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1880\u001b[0m             \u001b[0mloop_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1881\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mprocess_node\u001b[1;34m(self, fgraph, node, lopt)\u001b[0m\n\u001b[0;32m   1775\u001b[0m                 self.failure_callback(e, self,\n\u001b[0;32m   1776\u001b[0m                                       \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1777\u001b[1;33m                                       lopt, node)\n\u001b[0m\u001b[0;32m   1778\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1779\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mwarn_inplace\u001b[1;34m(exc, nav, repl_pairs, local_opt, node)\u001b[0m\n\u001b[0;32m   1671\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInconsistencyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1672\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1673\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mNavigatorOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnav\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl_pairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ritraina\\Anaconda2.4\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mwarn\u001b[1;34m(exc, nav, repl_pairs, local_opt, node)\u001b[0m\n\u001b[0;32m   1657\u001b[0m             \u001b[1;31m# We always crash on AssertionError because something may be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1658\u001b[0m             \u001b[1;31m# seriously wrong if such an exception is raised.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both \"conv_dnn\" and \"conv_gemm\" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against?"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 88.48%\n",
    "Again, there is a lot of opportunity for further optimization, such as the use of deeper and/or larger convolutional layers. One interesting idea is to set the max pooling layer to use an input length of 500. This would compress each feature map to a single 32 length vector and may boost performance.\n",
    "\n",
    "Summary\n",
    "\n",
    "In this post you discovered the IMDB sentiment analysis dataset for natural language processing.\n",
    "\n",
    "You learned how to develop deep learning models for sentiment analysis including:\n",
    "\n",
    "How to load and review the IMDB dataset within Keras.\n",
    "How to develop a large neural network model for sentiment analysis.\n",
    "How to develop a one-dimensional convolutional neural network model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
